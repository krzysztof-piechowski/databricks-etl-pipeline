{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d51510-506b-4f1f-8798-756fb0689c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Paths and Resource Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5aece4e-cd4d-406d-95e5-82ef8d0a81dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime\n",
    "import time, uuid, smtplib, traceback\n",
    "\n",
    "# --- PATHS ---\n",
    "silver_path = \"abfss://silver@storageaccpiechk.dfs.core.windows.net/orders/\"\n",
    "gold_fact_path = \"abfss://gold@storageaccpiechk.dfs.core.windows.net/FactOrders/\"\n",
    "gold_fact_table = \"databricks_cata.gold.FactOrders\"\n",
    "\n",
    "checkpoint_log_table = \"databricks_cata.gold.checkpoint_log\"\n",
    "checkpoint_table_name = 'FactOrders'\n",
    "\n",
    "batch_log_table = \"databricks_cata.gold.batch_log\"\n",
    "\n",
    "missing_dims_path = \"abfss://gold@storageaccpiechk.dfs.core.windows.net/FactOrders_MissingDims/\"\n",
    "staging_root = \"abfss://gold@storageaccpiechk.dfs.core.windows.net/FactOrders_Staging/\"\n",
    "\n",
    "# Gold dim table names\n",
    "dim_products_table = \"databricks_cata.gold.DimProducts\"\n",
    "dim_customers_table = \"databricks_cata.gold.DimCustomers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a820d939-fc88-4dd7-9e82-e1af92afabbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3f5c11-d2b6-41aa-af71-33387547ed5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ZORDER_COLS = [\"customer_sk\", \"product_sk\"]\n",
    "PARTITION_COLS = [\"year\", \"month\"]\n",
    "\n",
    "# tune for your environment\n",
    "TARGET_FILE_SIZE = 256 * 1024 * 1024  \n",
    "BROADCAST_THRESHOLD = 100 * 1024 * 1024  \n",
    "\n",
    "MAX_RETRIES = 2\n",
    "RETRY_BACKOFF_SEC = 10\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.targetFileSize\", str(TARGET_FILE_SIZE))\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",str(BROADCAST_THRESHOLD)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ea9e24-2cd7-4345-abb1-0b4d3b70eb76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Version & Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b667cc0b-e2c4-4e97-9290-bc71021760f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_current_silver_version():\n",
    "    hist = spark.sql(f\"DESCRIBE HISTORY delta.`{silver_path}`\")\n",
    "    maxv = hist.select(F.max(F.col('version'))).collect()[0][0]\n",
    "    return maxv\n",
    "\n",
    "def get_last_processed_version():\n",
    "    df = spark.sql(f\"SELECT last_processed_version FROM {checkpoint_log_table} WHERE table_name = '{checkpoint_table_name}'\")\n",
    "    if df.count() == 0:\n",
    "        return None\n",
    "    return df.collect()[0][0]\n",
    "\n",
    "def upsert_checkpoint(version):\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {checkpoint_log_table} AS c\n",
    "    USING (SELECT '{checkpoint_table_name}' AS table_name, {int(version)} AS last_processed_version, current_timestamp() AS last_processed_ts) AS s\n",
    "    ON c.table_name = s.table_name\n",
    "    WHEN MATCHED THEN UPDATE SET c.last_processed_version = s.last_processed_version, c.last_processed_ts = s.last_processed_ts\n",
    "    WHEN NOT MATCHED THEN INSERT (table_name, last_processed_version, last_processed_ts) VALUES (s.table_name, s.last_processed_version, s.last_processed_ts)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f02673-5da0-445c-9ddd-8911f66f605c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Batch Logging & ID Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd32c58b-b330-4976-855c-f2b71988db29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_batch_start(batch_id, starting_version, ending_version):\n",
    "    spark.sql(f\"INSERT INTO {batch_log_table} (batch_id, table_name, starting_version, ending_version, row_count, status, started_ts) VALUES ('{batch_id}', '{checkpoint_table_name}', {starting_version}, {ending_version}, 0, 'RUNNING', current_timestamp())\")\n",
    "\n",
    "def log_batch_end(batch_id, row_count, status='SUCCESS', error_msg=None):\n",
    "    err = f\"'{error_msg.replace(\"'\",\"\\''\")}'\" if error_msg else 'NULL'\n",
    "    spark.sql(f\"UPDATE {batch_log_table} SET row_count = {int(row_count)}, status = '{status}', finished_ts = current_timestamp(), error_msg = {err} WHERE batch_id = '{batch_id}'\")\n",
    "\n",
    "def generate_batch_id(prefix=\"FactOrders\", attempt_num=None):\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if attempt_num is not None:\n",
    "        return f\"{prefix}_{timestamp}_try{attempt_num}\"\n",
    "    else:\n",
    "        return f\"{prefix}_{timestamp}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd2aa07-de5b-4374-8468-2a3ce3e41926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Creation & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94943e40-8223-4851-9aba-feb73973372f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_clause = \", \".join(PARTITION_COLS)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {gold_fact_table} (\n",
    "  order_id STRING,\n",
    "  order_date DATE,\n",
    "  year INT,\n",
    "  month INT,\n",
    "  customer_sk BIGINT,\n",
    "  product_sk BIGINT,\n",
    "  quantity DOUBLE,\n",
    "  total_amount DOUBLE,\n",
    "  load_ts TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY ({partition_clause})\n",
    "LOCATION '{gold_fact_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb58708e-d29e-492c-92f8-c00af4850fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Main Processing Loop with Retry and Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1710d2-fb98-426f-93af-05a3f7cfeaed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 starting. Batch id: FactOrders_20251020_092103_try1\nLast processed version: 1\nCurrent silver version: 3\nReading CDF starting from version 2 to 3\nBatch contains 10 records to process.\nRunning OPTIMIZE + ZORDER on FactOrders (this may be slow depending on volume).\nBatch processed successfully.\nDone.\n"
     ]
    }
   ],
   "source": [
    "attempt = 0\n",
    "while attempt < MAX_RETRIES:\n",
    "    attempt += 1\n",
    "    batch_id = generate_batch_id(prefix=checkpoint_table_name, attempt_num=attempt)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Attempt {attempt} starting. Batch id: {batch_id}\")\n",
    "\n",
    "        last_version = get_last_processed_version()\n",
    "        current_version = get_current_silver_version()\n",
    "\n",
    "        print(f\"Last processed version: {last_version}\")\n",
    "        print(f\"Current silver version: {current_version}\")\n",
    "\n",
    "        if last_version is not None and last_version >= current_version:\n",
    "            print(\"No new changes to process.\")\n",
    "            break\n",
    "\n",
    "        starting_version = 0 if last_version is None else int(last_version) + 1\n",
    "        ending_version = current_version\n",
    "\n",
    "        # Skip if already succeeded for this range\n",
    "        existing = spark.sql(f\"\"\"\n",
    "            SELECT batch_id FROM {batch_log_table}\n",
    "            WHERE table_name = '{checkpoint_table_name}'\n",
    "                AND starting_version = {starting_version}\n",
    "                AND ending_version = {ending_version}\n",
    "                AND status = 'SUCCESS'\n",
    "        \"\"\")\n",
    "        if existing.count() > 0:\n",
    "            print(\"This version range was already processed successfully. Exiting.\")\n",
    "            break\n",
    "\n",
    "        # Create batch log\n",
    "        log_batch_start(batch_id, starting_version, ending_version)\n",
    "\n",
    "        print(f\"Reading CDF starting from version {starting_version} to {ending_version}\")\n",
    "\n",
    "        # Read CDF from silver\n",
    "        cdf_df = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingVersion\", str(starting_version))\n",
    "            .option(\"endingVersion\", str(ending_version))\n",
    "            .load(silver_path)\n",
    "        )\n",
    "\n",
    "        # Filter inserts\n",
    "        inserts_df = cdf_df.filter(F.col(\"_change_type\") == 'insert')\n",
    "        if inserts_df.limit(1).count() == 0:\n",
    "\n",
    "            print(\"No insert changes in CDF range. Updating checkpoint and exiting.\")\n",
    "            upsert_checkpoint(ending_version)\n",
    "            # mark batch as SUCCESS with 0 rows\n",
    "            log_batch_end(batch_id, 0, status='SUCCESS')\n",
    "            break\n",
    "\n",
    "        # Join with current dimension tables to get surrogate keys\n",
    "        dim_products = spark.table(dim_products_table).filter(\"is_current = true\").select(\"product_id\", \"product_sk\")\n",
    "        dim_customers = spark.table(dim_customers_table).filter(\"is_current = true\").select(\"customer_id\", \"customer_sk\")\n",
    "\n",
    "        transformed = (\n",
    "            inserts_df\n",
    "             .withColumn(\"order_date\", F.to_date(F.col(\"order_date\")))\n",
    "             .withColumn(\"year\", F.year(F.col(\"order_date\")))\n",
    "             .withColumn(\"month\", F.month(F.col(\"order_date\")))\n",
    "             .join(F.broadcast(dim_products), on=[\"product_id\"], how=\"left\")\n",
    "             .join(F.broadcast(dim_customers), on=[\"customer_id\"], how=\"left\")\n",
    "             .select(\n",
    "                F.col(\"order_id\").alias(\"order_id\"),\n",
    "                F.col(\"order_date\"),\n",
    "                F.col(\"year\"),\n",
    "                F.col(\"month\"),\n",
    "                F.col(\"customer_sk\"),\n",
    "                F.col(\"product_sk\"),\n",
    "                F.col(\"quantity\"),\n",
    "                F.col(\"total_amount\"),\n",
    "                F.current_timestamp().alias(\"load_ts\")\n",
    "            )\n",
    "            .withColumn(\"batch_id\", F.lit(batch_id))\n",
    "        )\n",
    "\n",
    "        # Handle missing surrogate keys: write unmatched to missing_dims_table\n",
    "        missing_dims = transformed.filter(F.col(\"customer_sk\").isNull() | F.col(\"product_sk\").isNull())\n",
    "        missing_count = missing_dims.count()\n",
    "        if missing_count > 0:\n",
    "            print(f\"WARNING: {missing_count} rows with missing surrogate keys. Writing to staging for manual review.\")\n",
    "            missing_dims.write.format(\"delta\").mode(\"append\").save(missing_dims_path)\n",
    "\n",
    "        # Idempotent publish: write to staging and MERGE into gold fact table\n",
    "        staging_path = staging_root + f\"batch_{batch_id}/\"\n",
    "        transformed.write.format(\"delta\").mode(\"overwrite\").save(staging_path)\n",
    "\n",
    "        # MERGE: insert only when order_id not exists (idempotent)\n",
    "        delta_fact = DeltaTable.forPath(spark, gold_fact_path)\n",
    "        staging_df = spark.read.format(\"delta\").load(staging_path)\n",
    "\n",
    "        appended_count = staging_df.count()\n",
    "        print(f\"Batch contains {appended_count} records to process.\")       \n",
    "\n",
    "        # Broadcast the small staging table so Spark can push down year/month filters\n",
    "        # and leverage partition pruning on the fact table (partitioned by year, month).\n",
    "\n",
    "        # Can be turned off if the staging table is too large.\n",
    "        delta_fact.alias(\"t\").merge(\n",
    "            #staging_df.alias(\"s\"),\n",
    "            F.broadcast(staging_df).alias(\"s\"),\n",
    "            \"t.order_id = s.order_id AND t.year = s.year AND t.month = s.month\"\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "        # Update checkpoint and batch log\n",
    "        upsert_checkpoint(ending_version)\n",
    "        log_batch_end(batch_id, appended_count, status='SUCCESS')\n",
    "\n",
    "        # Optimize gold fact table\n",
    "        print(\"Running OPTIMIZE + ZORDER on FactOrders (this may be slow depending on volume).\")\n",
    "        spark.sql(f\"OPTIMIZE delta.`{gold_fact_path}` ZORDER BY ({', '.join(ZORDER_COLS)})\")\n",
    "\n",
    "        print(\"Batch processed successfully.\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Handle errors\n",
    "        error_trace = traceback.format_exc()  \n",
    "        print(f\"Error during batch {batch_id}: {e}\")\n",
    "        print(error_trace)  \n",
    "        \n",
    "        log_batch_end(batch_id, 0, status='FAILED', error_msg=error_trace)\n",
    "        if attempt < MAX_RETRIES:\n",
    "            sleep_time = RETRY_BACKOFF_SEC * (2 ** (attempt - 1))\n",
    "            print(f\"Retrying after {sleep_time} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "            continue\n",
    "        else:\n",
    "            # Exhausted retries, re-raise\n",
    "            raise\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d538488-68d5-4f28-8a8c-9f2991357fa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Checkpoint Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4928c9dc-96f9-4969-9295-2d24a3994f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>table_name</th><th>last_processed_version</th><th>last_processed_ts</th></tr></thead><tbody><tr><td>DimCustomers</td><td>3</td><td>2025-10-20T09:19:37.903199Z</td></tr><tr><td>DimProducts</td><td>3</td><td>2025-10-20T09:19:47.53992Z</td></tr><tr><td>FactOrders</td><td>3</td><td>2025-10-20T09:21:17.258511Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DimCustomers",
         3,
         "2025-10-20T09:19:37.903199Z"
        ],
        [
         "DimProducts",
         3,
         "2025-10-20T09:19:47.53992Z"
        ],
        [
         "FactOrders",
         3,
         "2025-10-20T09:21:17.258511Z"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "table_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "last_processed_version",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "last_processed_ts",
            "nullable": true,
            "type": "timestamp"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 17
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_processed_version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "last_processed_ts",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select *\n",
    "from databricks_cata.gold.checkpoint_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030328cb-2d1c-413e-be25-0584d1b03b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Batch Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d056e22-4df6-40a9-8905-bca091bbfb78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>batch_id</th><th>table_name</th><th>starting_version</th><th>ending_version</th><th>row_count</th><th>status</th><th>started_ts</th><th>finished_ts</th><th>error_msg</th></tr></thead><tbody><tr><td>DimCustomers_20251020_090909_try1</td><td>DimCustomers</td><td>0</td><td>1</td><td>1990</td><td>SUCCESS</td><td>2025-10-20T09:09:15.615289Z</td><td>2025-10-20T09:09:28.608432Z</td><td>null</td></tr><tr><td>DimCustomers_20251020_091926_try1</td><td>DimCustomers</td><td>2</td><td>3</td><td>10</td><td>SUCCESS</td><td>2025-10-20T09:19:29.801783Z</td><td>2025-10-20T09:19:41.874974Z</td><td>null</td></tr><tr><td>DimProducts_20251020_090913_try1</td><td>DimProducts</td><td>0</td><td>1</td><td>490</td><td>SUCCESS</td><td>2025-10-20T09:09:16.627671Z</td><td>2025-10-20T09:09:29.799232Z</td><td>null</td></tr><tr><td>DimProducts_20251020_091934_try1</td><td>DimProducts</td><td>2</td><td>3</td><td>10</td><td>SUCCESS</td><td>2025-10-20T09:19:38.46511Z</td><td>2025-10-20T09:19:50.817749Z</td><td>null</td></tr><tr><td>FactOrders_20251020_091037_try1</td><td>FactOrders</td><td>0</td><td>1</td><td>9990</td><td>SUCCESS</td><td>2025-10-20T09:10:40.648664Z</td><td>2025-10-20T09:11:01.080474Z</td><td>null</td></tr><tr><td>FactOrders_20251020_092103_try1</td><td>FactOrders</td><td>2</td><td>3</td><td>10</td><td>SUCCESS</td><td>2025-10-20T09:21:06.468513Z</td><td>2025-10-20T09:21:19.649757Z</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DimCustomers_20251020_090909_try1",
         "DimCustomers",
         0,
         1,
         1990,
         "SUCCESS",
         "2025-10-20T09:09:15.615289Z",
         "2025-10-20T09:09:28.608432Z",
         null
        ],
        [
         "DimCustomers_20251020_091926_try1",
         "DimCustomers",
         2,
         3,
         10,
         "SUCCESS",
         "2025-10-20T09:19:29.801783Z",
         "2025-10-20T09:19:41.874974Z",
         null
        ],
        [
         "DimProducts_20251020_090913_try1",
         "DimProducts",
         0,
         1,
         490,
         "SUCCESS",
         "2025-10-20T09:09:16.627671Z",
         "2025-10-20T09:09:29.799232Z",
         null
        ],
        [
         "DimProducts_20251020_091934_try1",
         "DimProducts",
         2,
         3,
         10,
         "SUCCESS",
         "2025-10-20T09:19:38.46511Z",
         "2025-10-20T09:19:50.817749Z",
         null
        ],
        [
         "FactOrders_20251020_091037_try1",
         "FactOrders",
         0,
         1,
         9990,
         "SUCCESS",
         "2025-10-20T09:10:40.648664Z",
         "2025-10-20T09:11:01.080474Z",
         null
        ],
        [
         "FactOrders_20251020_092103_try1",
         "FactOrders",
         2,
         3,
         10,
         "SUCCESS",
         "2025-10-20T09:21:06.468513Z",
         "2025-10-20T09:21:19.649757Z",
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "batch_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "table_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "starting_version",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "ending_version",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "row_count",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "status",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "started_ts",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "finished_ts",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "error_msg",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 18
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "batch_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "starting_version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ending_version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "row_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "started_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "finished_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "error_msg",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select *\n",
    "from databricks_cata.gold.batch_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb52cdf9-bacd-4dad-bb3a-c6375b7c7770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8549198551271151,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Fact_Orders",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}