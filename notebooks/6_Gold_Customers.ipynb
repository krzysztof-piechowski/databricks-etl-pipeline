{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7219d293-bd04-4f7a-9d2a-fa6882e91058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Paths and Resource Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae19dd5-9062-45b2-9f0d-545068fee372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "import time, uuid\n",
    "import traceback\n",
    "\n",
    "# --- PATHS ---\n",
    "silver_path = \"abfss://silver@storageaccpiechk.dfs.core.windows.net/customers/\"\n",
    "gold_path   = \"abfss://gold@storageaccpiechk.dfs.core.windows.net/DimCustomers/\"\n",
    "gold_dim_table = \"databricks_cata.gold.DimCustomers\"\n",
    "\n",
    "checkpoint_log_table = \"databricks_cata.gold.checkpoint_log\"\n",
    "checkpoint_table_name = \"DimCustomers\"\n",
    "\n",
    "batch_log_table = \"databricks_cata.gold.batch_log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a363697a-00b3-45d6-a485-bd247bcac21f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df552731-5cd9-4de9-9e9d-245f443207c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ZORDER_COLS = \"customer_sk\"\n",
    "\n",
    "# tune for your environment\n",
    "TARGET_FILE_SIZE = 128 * 1024 * 1024\n",
    "MAX_RETRIES = 2\n",
    "RETRY_BACKOFF_SEC = 10\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.targetFileSize\", str(TARGET_FILE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2aa62b-193e-4fd5-bd0a-4c2bb471329f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Version & Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49761072-6820-4b07-b029-1718da849e2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_current_silver_version():\n",
    "    hist = spark.sql(f\"DESCRIBE HISTORY delta.`{silver_path}`\")\n",
    "    maxv = hist.select(F.max(F.col('version'))).collect()[0][0]\n",
    "    return maxv\n",
    "\n",
    "def get_last_processed_version():\n",
    "    df = spark.sql(f\"SELECT last_processed_version FROM {checkpoint_log_table} WHERE table_name = '{checkpoint_table_name}'\")\n",
    "    if df.count() == 0:\n",
    "        return None\n",
    "    return df.collect()[0][0]\n",
    "\n",
    "def upsert_checkpoint(version):\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {checkpoint_log_table} AS c\n",
    "    USING (SELECT '{checkpoint_table_name}' AS table_name, {int(version)} AS last_processed_version, current_timestamp() AS last_processed_ts) AS s\n",
    "    ON c.table_name = s.table_name\n",
    "    WHEN MATCHED THEN UPDATE SET c.last_processed_version = s.last_processed_version, c.last_processed_ts = s.last_processed_ts\n",
    "    WHEN NOT MATCHED THEN INSERT (table_name, last_processed_version, last_processed_ts) VALUES (s.table_name, s.last_processed_version, s.last_processed_ts)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57235b2-8af2-43ae-bd0e-ed4ae00cb4e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Batch Logging & ID Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579595a9-6fbe-427e-87a7-f31842f4a250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_batch_start(batch_id, starting_version, ending_version):\n",
    "    spark.sql(f\"INSERT INTO {batch_log_table} (batch_id, table_name, starting_version, ending_version, row_count, status, started_ts) VALUES ('{batch_id}', '{checkpoint_table_name}', {starting_version}, {ending_version}, 0, 'RUNNING', current_timestamp())\")\n",
    "\n",
    "def log_batch_end(batch_id, row_count, status='SUCCESS', error_msg=None):\n",
    "    err = f\"'{error_msg.replace(\"'\",\"\\''\")}'\" if error_msg else 'NULL'\n",
    "    spark.sql(f\"UPDATE {batch_log_table} SET row_count = {int(row_count)}, status = '{status}', finished_ts = current_timestamp(), error_msg = {err} WHERE batch_id = '{batch_id}'\")\n",
    "\n",
    "def generate_batch_id(prefix=\"DimCustomers\", attempt_num=None):\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if attempt_num is not None:\n",
    "        return f\"{prefix}_{timestamp}_try{attempt_num}\"\n",
    "    else:\n",
    "        return f\"{prefix}_{timestamp}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6562a0f-5bb3-47f7-a2f8-cdda13a45458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Creation & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d4d759-bc1f-43a1-a3ff-67d625dca6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {gold_dim_table} (\n",
    "    customer_sk BIGINT GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1),\n",
    "    customer_id STRING,\n",
    "    email STRING,\n",
    "    city STRING,\n",
    "    state STRING,\n",
    "    email_domain STRING,\n",
    "    full_name STRING,\n",
    "    hash_value STRING,\n",
    "    last_update_ts TIMESTAMP,\n",
    "    valid_from TIMESTAMP,\n",
    "    valid_to TIMESTAMP,\n",
    "    is_current BOOLEAN\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '{gold_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4572859f-b1f6-44f4-bee0-2d6043f0d4b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Main Processing Loop with Retry and Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f06e2df-e14a-4d5e-8d3f-c4656c2c9c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 - Batch ID: DimCustomers_20251020_091926_try1\nLast processed Silver version: 1\nCurrent Silver version: 3\nReading CDF from version 2 to 3 ...\nUpserted 10 records to Gold DimCustomers\nRunning OPTIMIZE + ZORDER ...\nBatch completed successfully.\nDimCustomers pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "attempt = 0\n",
    "while attempt < MAX_RETRIES:\n",
    "    attempt += 1\n",
    "    batch_id = generate_batch_id(prefix=checkpoint_table_name, attempt_num=attempt)\n",
    "\n",
    "    try:\n",
    "        print(f\"Attempt {attempt} - Batch ID: {batch_id}\")\n",
    "\n",
    "        last_version = get_last_processed_version()\n",
    "        current_version = get_current_silver_version()\n",
    "\n",
    "        print(f\"Last processed Silver version: {last_version}\")\n",
    "        print(f\"Current Silver version: {current_version}\")\n",
    "\n",
    "        if last_version is not None and last_version >= current_version:\n",
    "            print(\"No new changes to process.\")\n",
    "            break\n",
    "\n",
    "        starting_version = 0 if last_version is None else int(last_version) + 1\n",
    "        ending_version = current_version\n",
    "\n",
    "        # Skip if already succeeded for this range\n",
    "        existing = spark.sql(f\"\"\"\n",
    "            SELECT batch_id FROM {batch_log_table}\n",
    "            WHERE table_name = '{checkpoint_table_name}'\n",
    "              AND starting_version = {starting_version}\n",
    "              AND ending_version = {ending_version}\n",
    "              AND status = 'SUCCESS'\n",
    "        \"\"\")\n",
    "        if existing.count() > 0:\n",
    "            print(\"This version range already processed. Skipping.\")\n",
    "            break\n",
    "\n",
    "        # Create batch log\n",
    "        log_batch_start(batch_id, starting_version, ending_version)\n",
    "\n",
    "        print(f\"Reading CDF from version {starting_version} to {ending_version} ...\")\n",
    "\n",
    "        # Read CDF from silver\n",
    "        cdf_df = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingVersion\", str(starting_version))\n",
    "            .option(\"endingVersion\", str(ending_version))\n",
    "            .load(silver_path)\n",
    "        )\n",
    "\n",
    "        # Filter inserts and post-images\n",
    "        changed_df = cdf_df.filter(F.col(\"_change_type\").isin(\"insert\", \"update_postimage\"))\n",
    "        if changed_df.limit(1).count() == 0:\n",
    "            print(\"No changes detected in Silver CDF.\")\n",
    "            upsert_checkpoint(ending_version)\n",
    "            log_batch_end(batch_id, 0, status='SUCCESS')\n",
    "            break\n",
    "\n",
    "        # Prepare SCD2 records\n",
    "        new_records = (\n",
    "            changed_df\n",
    "            .select(\n",
    "                \"customer_id\", \"email\", \"city\", \"state\", \"email_domain\", \"full_name\",\n",
    "                \"hash_value\", \"last_update_ts\"\n",
    "            )\n",
    "            .withColumn(\"valid_from\", F.current_timestamp())\n",
    "            .withColumn(\"valid_to\", F.lit(None).cast(\"timestamp\"))\n",
    "            .withColumn(\"is_current\", F.lit(True))\n",
    "        )\n",
    "\n",
    "        delta_gold = DeltaTable.forPath(spark, gold_path)\n",
    "\n",
    "        # Expire old versions\n",
    "        delta_gold.alias(\"g\") \\\n",
    "            .merge(\n",
    "                new_records.select(\"customer_id\").alias(\"s\"),\n",
    "                \"g.customer_id = s.customer_id AND g.is_current = true\"\n",
    "            ) \\\n",
    "            .whenMatchedUpdate(set={\n",
    "                \"valid_to\": F.current_timestamp(),\n",
    "                \"is_current\": F.lit(False)\n",
    "            }) \\\n",
    "            .execute()\n",
    "\n",
    "        # Append new versions\n",
    "        new_records.write.format(\"delta\").mode(\"append\").save(gold_path)\n",
    "\n",
    "        updated_count = new_records.count()\n",
    "        print(f\"Upserted {updated_count} records to Gold DimCustomers\")\n",
    "\n",
    "        # Update checkpoint and batch log\n",
    "        upsert_checkpoint(ending_version)\n",
    "        log_batch_end(batch_id, updated_count, status='SUCCESS')\n",
    "\n",
    "        # Optimize gold table\n",
    "        print(\"Running OPTIMIZE + ZORDER ...\")\n",
    "        spark.sql(f\"OPTIMIZE delta.`{gold_path}` ZORDER BY {ZORDER_COLS}\")\n",
    "\n",
    "        print(\"Batch completed successfully.\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        # Handle errors\n",
    "        error_trace = traceback.format_exc() \n",
    "        print(f\"Error during batch {batch_id}: {e}\")\n",
    "        print(error_trace) \n",
    "        \n",
    "        log_batch_end(batch_id, 0, status='FAILED', error_msg=error_trace)\n",
    "\n",
    "        if attempt < MAX_RETRIES:\n",
    "            sleep_time = RETRY_BACKOFF_SEC * (2 ** (attempt - 1))\n",
    "            print(f\"Retrying after {sleep_time} sec...\")\n",
    "            time.sleep(sleep_time)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(\"DimCustomers pipeline finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b34e9e3-e6e9-4371-b63c-901384759c86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Checkpoint Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c775cbcd-4f81-4145-983c-0d9a64d827f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>table_name</th><th>last_processed_version</th><th>last_processed_ts</th></tr></thead><tbody><tr><td>DimCustomers</td><td>3</td><td>2025-10-20T09:19:37.903199Z</td></tr><tr><td>DimProducts</td><td>3</td><td>2025-10-20T09:19:47.53992Z</td></tr><tr><td>FactOrders</td><td>1</td><td>2025-10-20T09:10:59.105358Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DimCustomers",
         3,
         "2025-10-20T09:19:37.903199Z"
        ],
        [
         "DimProducts",
         3,
         "2025-10-20T09:19:47.53992Z"
        ],
        [
         "FactOrders",
         1,
         "2025-10-20T09:10:59.105358Z"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "table_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "last_processed_version",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "last_processed_ts",
            "nullable": true,
            "type": "timestamp"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 15
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_processed_version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "last_processed_ts",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select *\n",
    "from databricks_cata.gold.checkpoint_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b88e3e-46a1-49b1-9ed8-b7d7a5406464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Batch Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4db7beab-a1fe-4eea-824a-8421e0dc8a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>batch_id</th><th>table_name</th><th>starting_version</th><th>ending_version</th><th>row_count</th><th>status</th><th>started_ts</th><th>finished_ts</th><th>error_msg</th></tr></thead><tbody><tr><td>DimCustomers_20251020_090909_try1</td><td>DimCustomers</td><td>0</td><td>1</td><td>1990</td><td>SUCCESS</td><td>2025-10-20T09:09:15.615289Z</td><td>2025-10-20T09:09:28.608432Z</td><td>null</td></tr><tr><td>DimCustomers_20251020_091926_try1</td><td>DimCustomers</td><td>2</td><td>3</td><td>10</td><td>SUCCESS</td><td>2025-10-20T09:19:29.801783Z</td><td>2025-10-20T09:19:41.874974Z</td><td>null</td></tr><tr><td>DimProducts_20251020_090913_try1</td><td>DimProducts</td><td>0</td><td>1</td><td>490</td><td>SUCCESS</td><td>2025-10-20T09:09:16.627671Z</td><td>2025-10-20T09:09:29.799232Z</td><td>null</td></tr><tr><td>DimProducts_20251020_091934_try1</td><td>DimProducts</td><td>2</td><td>3</td><td>10</td><td>SUCCESS</td><td>2025-10-20T09:19:38.46511Z</td><td>2025-10-20T09:19:50.817749Z</td><td>null</td></tr><tr><td>FactOrders_20251020_091037_try1</td><td>FactOrders</td><td>0</td><td>1</td><td>9990</td><td>SUCCESS</td><td>2025-10-20T09:10:40.648664Z</td><td>2025-10-20T09:11:01.080474Z</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DimCustomers_20251020_090909_try1",
         "DimCustomers",
         0,
         1,
         1990,
         "SUCCESS",
         "2025-10-20T09:09:15.615289Z",
         "2025-10-20T09:09:28.608432Z",
         null
        ],
        [
         "DimCustomers_20251020_091926_try1",
         "DimCustomers",
         2,
         3,
         10,
         "SUCCESS",
         "2025-10-20T09:19:29.801783Z",
         "2025-10-20T09:19:41.874974Z",
         null
        ],
        [
         "DimProducts_20251020_090913_try1",
         "DimProducts",
         0,
         1,
         490,
         "SUCCESS",
         "2025-10-20T09:09:16.627671Z",
         "2025-10-20T09:09:29.799232Z",
         null
        ],
        [
         "DimProducts_20251020_091934_try1",
         "DimProducts",
         2,
         3,
         10,
         "SUCCESS",
         "2025-10-20T09:19:38.46511Z",
         "2025-10-20T09:19:50.817749Z",
         null
        ],
        [
         "FactOrders_20251020_091037_try1",
         "FactOrders",
         0,
         1,
         9990,
         "SUCCESS",
         "2025-10-20T09:10:40.648664Z",
         "2025-10-20T09:11:01.080474Z",
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "batch_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "table_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "starting_version",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "ending_version",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "row_count",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "status",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "started_ts",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "finished_ts",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "error_msg",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 16
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "batch_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "starting_version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ending_version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "row_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "started_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "finished_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "error_msg",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%sql\n",
    "select *\n",
    "from databricks_cata.gold.batch_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68aa0e4b-b8af-4e3d-9494-1ed60f455697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7459374578409114,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_Customers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}