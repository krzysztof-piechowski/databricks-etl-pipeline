{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728f3306-80a7-47a7-b0f1-881270c1eba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Paths and Resource Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab312219-1a19-4b33-aaf4-80f797afad6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "import time, uuid\n",
    "import traceback\n",
    "\n",
    "# --- PATHS ---\n",
    "silver_path = \"abfss://silver@storageaccpiechk.dfs.core.windows.net/products/\"\n",
    "gold_path  = \"abfss://gold@storageaccpiechk.dfs.core.windows.net/DimProducts/\"\n",
    "gold_dim_table = \"databricks_cata.gold.DimProducts\"\n",
    "\n",
    "checkpoint_log_table = \"databricks_cata.gold.checkpoint_log\"\n",
    "checkpoint_table_name = \"DimProducts\"\n",
    "\n",
    "batch_log_table = \"databricks_cata.gold.batch_log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df61ae4f-7e09-4d86-8158-296d0b5b274f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7ee79e-a9af-4e83-8214-8301e9e46d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ZORDER_COLS = \"product_sk\"\n",
    "\n",
    "# tune for your environment\n",
    "TARGET_FILE_SIZE = 128 * 1024 * 1024\n",
    "MAX_RETRIES = 2\n",
    "RETRY_BACKOFF_SEC = 10\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.targetFileSize\", str(TARGET_FILE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ac34bc-8967-400f-bfeb-c24144d27c3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Version & Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afb9882-bc88-4df8-bebe-4e2c76c6343a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_current_silver_version():\n",
    "    hist = spark.sql(f\"DESCRIBE HISTORY delta.`{silver_path}`\")\n",
    "    maxv = hist.select(F.max(F.col('version'))).collect()[0][0]\n",
    "    return maxv\n",
    "\n",
    "def get_last_processed_version():\n",
    "    df = spark.sql(f\"SELECT last_processed_version FROM {checkpoint_log_table} WHERE table_name = '{checkpoint_table_name}'\")\n",
    "    if df.count() == 0:\n",
    "        return None\n",
    "    return df.collect()[0][0]\n",
    "\n",
    "def upsert_checkpoint(version):\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {checkpoint_log_table} AS c\n",
    "    USING (SELECT '{checkpoint_table_name}' AS table_name, {int(version)} AS last_processed_version, current_timestamp() AS last_processed_ts) AS s\n",
    "    ON c.table_name = s.table_name\n",
    "    WHEN MATCHED THEN UPDATE SET c.last_processed_version = s.last_processed_version, c.last_processed_ts = s.last_processed_ts\n",
    "    WHEN NOT MATCHED THEN INSERT (table_name, last_processed_version, last_processed_ts) VALUES (s.table_name, s.last_processed_version, s.last_processed_ts)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bcdf55f-a819-4652-88c1-e9ef5002e93f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Batch Logging & ID Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cdb0c9-d405-4a9e-b0b3-cccd4beb30d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_batch_start(batch_id, starting_version, ending_version):\n",
    "    spark.sql(f\"INSERT INTO {batch_log_table} (batch_id, table_name, starting_version, ending_version, row_count, status, started_ts) VALUES ('{batch_id}', '{checkpoint_table_name}', {starting_version}, {ending_version}, 0, 'RUNNING', current_timestamp())\")\n",
    "\n",
    "def log_batch_end(batch_id, row_count, status='SUCCESS', error_msg=None):\n",
    "    err = f\"'{error_msg.replace(\"'\",\"\\''\")}'\" if error_msg else 'NULL'\n",
    "    spark.sql(f\"UPDATE {batch_log_table} SET row_count = {int(row_count)}, status = '{status}', finished_ts = current_timestamp(), error_msg = {err} WHERE batch_id = '{batch_id}'\")\n",
    "\n",
    "def generate_batch_id(prefix=\"DimProducts\", attempt_num=None):\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if attempt_num is not None:\n",
    "        return f\"{prefix}_{timestamp}_try{attempt_num}\"\n",
    "    else:\n",
    "        return f\"{prefix}_{timestamp}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be0631a7-c1c5-4e3b-823f-c5d1abf82452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Creation & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "714c6403-c0b9-4d51-9a71-ffcf0f72afce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {gold_dim_table} (\n",
    "    product_sk BIGINT GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1),\n",
    "    product_id STRING,\n",
    "    product_name STRING,\n",
    "    category STRING,\n",
    "    brand STRING,\n",
    "    price DOUBLE,\n",
    "    hash_value STRING,\n",
    "    last_update_ts TIMESTAMP,\n",
    "    valid_from TIMESTAMP,\n",
    "    valid_to TIMESTAMP,\n",
    "    is_current BOOLEAN\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '{gold_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b997a61c-4e25-4795-aec8-f78c744dc34f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Main Processing Loop with Retry and Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9d39e5-e83f-484b-94bc-dfbcf17aa528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 - Batch ID: DimProducts_20251020_091934_try1\nLast processed Silver version: 1\nCurrent Silver version: 3\nReading CDF from version 2 to 3 ...\nUpserted 10 records to Gold DimProducts\nRunning OPTIMIZE + ZORDER ...\nBatch completed successfully.\nDimProducts pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "attempt = 0\n",
    "while attempt < MAX_RETRIES:\n",
    "    attempt += 1\n",
    "    batch_id = generate_batch_id(prefix=checkpoint_table_name, attempt_num=attempt)\n",
    "\n",
    "    try:\n",
    "        print(f\"Attempt {attempt} - Batch ID: {batch_id}\")\n",
    "\n",
    "        last_version = get_last_processed_version()\n",
    "        current_version = get_current_silver_version()\n",
    "\n",
    "        print(f\"Last processed Silver version: {last_version}\")\n",
    "        print(f\"Current Silver version: {current_version}\")\n",
    "\n",
    "        if last_version is not None and last_version >= current_version:\n",
    "            print(\"No new changes to process.\")\n",
    "            break\n",
    "\n",
    "        starting_version = 0 if last_version is None else int(last_version) + 1\n",
    "        ending_version = current_version\n",
    "\n",
    "        # Skip if already succeeded for this range\n",
    "        existing = spark.sql(f\"\"\"\n",
    "            SELECT batch_id FROM {batch_log_table}\n",
    "            WHERE table_name = '{checkpoint_table_name}'\n",
    "              AND starting_version = {starting_version}\n",
    "              AND ending_version = {ending_version}\n",
    "              AND status = 'SUCCESS'\n",
    "        \"\"\")\n",
    "        if existing.count() > 0:\n",
    "            print(\"This version range already processed. Skipping.\")\n",
    "            break\n",
    "\n",
    "        # Create batch log\n",
    "        log_batch_start(batch_id, starting_version, ending_version)\n",
    "\n",
    "        print(f\"Reading CDF from version {starting_version} to {ending_version} ...\")\n",
    "\n",
    "        # Read CDF from silver\n",
    "        cdf_df = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingVersion\", str(starting_version))\n",
    "            .option(\"endingVersion\", str(ending_version))\n",
    "            .load(silver_path)\n",
    "        )\n",
    "\n",
    "        # Filter inserts and post-images\n",
    "        changed_df = cdf_df.filter(F.col(\"_change_type\").isin(\"insert\", \"update_postimage\"))\n",
    "        if changed_df.limit(1).count() == 0:\n",
    "            print(\"No changes detected in Silver CDF.\")\n",
    "            upsert_checkpoint(ending_version)\n",
    "            log_batch_end(batch_id, 0, status='SUCCESS')\n",
    "            break\n",
    "\n",
    "        # Prepare SCD2 records\n",
    "        new_records = (\n",
    "            changed_df\n",
    "            .select(\n",
    "                \"product_id\", \"product_name\", \"category\", \"brand\",\n",
    "                \"price\", \"hash_value\", \"last_update_ts\"\n",
    "            )\n",
    "            .withColumn(\"valid_from\", F.current_timestamp())\n",
    "            .withColumn(\"valid_to\", F.lit(None).cast(\"timestamp\"))\n",
    "            .withColumn(\"is_current\", F.lit(True))\n",
    "        )\n",
    "\n",
    "        delta_gold = DeltaTable.forPath(spark, gold_path)\n",
    "\n",
    "        # Expire old versions\n",
    "        delta_gold.alias(\"g\") \\\n",
    "            .merge(\n",
    "                new_records.select(\"product_id\").alias(\"s\"),\n",
    "                \"g.product_id = s.product_id AND g.is_current = true\"\n",
    "            ) \\\n",
    "            .whenMatchedUpdate(set={\n",
    "                \"valid_to\": F.current_timestamp(),\n",
    "                \"is_current\": F.lit(False)\n",
    "            }) \\\n",
    "            .execute()\n",
    "\n",
    "        # Append new versions\n",
    "        new_records.write.format(\"delta\").mode(\"append\").save(gold_path)\n",
    "\n",
    "        updated_count = new_records.count()\n",
    "        print(f\"Upserted {updated_count} records to Gold DimProducts\")\n",
    "\n",
    "        # Update checkpoint and batch log\n",
    "        upsert_checkpoint(ending_version)\n",
    "        log_batch_end(batch_id, updated_count, status='SUCCESS')\n",
    "\n",
    "        # Optimize gold table\n",
    "        print(\"Running OPTIMIZE + ZORDER ...\")\n",
    "        spark.sql(f\"OPTIMIZE delta.`{gold_path}` ZORDER BY {ZORDER_COLS}\")\n",
    "\n",
    "        print(\"Batch completed successfully.\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        # Handle errors\n",
    "        error_trace = traceback.format_exc()\n",
    "        print(f\"Error during batch {batch_id}: {e}\")\n",
    "        print(error_trace)\n",
    "        \n",
    "        log_batch_end(batch_id, 0, status='FAILED', error_msg=error_trace)\n",
    "\n",
    "        if attempt < MAX_RETRIES:\n",
    "            sleep_time = RETRY_BACKOFF_SEC * (2 ** (attempt - 1))\n",
    "            print(f\"Retrying after {sleep_time} sec...\")\n",
    "            time.sleep(sleep_time)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(\"DimProducts pipeline finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "738126e0-bd9e-4450-8db1-dd03aafaa345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Checkpoint Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c22d4dc-a8e5-465d-8645-ed36e1538898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>table_name</th><th>last_processed_version</th><th>last_processed_ts</th></tr></thead><tbody><tr><td>DimCustomers</td><td>3</td><td>2025-10-20T09:19:37.903199Z</td></tr><tr><td>DimProducts</td><td>3</td><td>2025-10-20T09:19:47.53992Z</td></tr><tr><td>FactOrders</td><td>1</td><td>2025-10-20T09:10:59.105358Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DimCustomers",
         3,
         "2025-10-20T09:19:37.903199Z"
        ],
        [
         "DimProducts",
         3,
         "2025-10-20T09:19:47.53992Z"
        ],
        [
         "FactOrders",
         1,
         "2025-10-20T09:10:59.105358Z"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "table_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "last_processed_version",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "last_processed_ts",
            "nullable": true,
            "type": "timestamp"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 15
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_processed_version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "last_processed_ts",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select *\n",
    "from databricks_cata.gold.checkpoint_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3842dd75-11b7-46c0-bcb7-b3e347f45f40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Batch Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d731940a-a631-4a88-9048-e35c08f1e64f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>batch_id</th><th>table_name</th><th>starting_version</th><th>ending_version</th><th>row_count</th><th>status</th><th>started_ts</th><th>finished_ts</th><th>error_msg</th></tr></thead><tbody><tr><td>DimCustomers_20251020_090909_try1</td><td>DimCustomers</td><td>0</td><td>1</td><td>1990</td><td>SUCCESS</td><td>2025-10-20T09:09:15.615289Z</td><td>2025-10-20T09:09:28.608432Z</td><td>null</td></tr><tr><td>DimCustomers_20251020_091926_try1</td><td>DimCustomers</td><td>2</td><td>3</td><td>10</td><td>SUCCESS</td><td>2025-10-20T09:19:29.801783Z</td><td>2025-10-20T09:19:41.874974Z</td><td>null</td></tr><tr><td>DimProducts_20251020_090913_try1</td><td>DimProducts</td><td>0</td><td>1</td><td>490</td><td>SUCCESS</td><td>2025-10-20T09:09:16.627671Z</td><td>2025-10-20T09:09:29.799232Z</td><td>null</td></tr><tr><td>DimProducts_20251020_091934_try1</td><td>DimProducts</td><td>2</td><td>3</td><td>10</td><td>SUCCESS</td><td>2025-10-20T09:19:38.46511Z</td><td>2025-10-20T09:19:50.817749Z</td><td>null</td></tr><tr><td>FactOrders_20251020_091037_try1</td><td>FactOrders</td><td>0</td><td>1</td><td>9990</td><td>SUCCESS</td><td>2025-10-20T09:10:40.648664Z</td><td>2025-10-20T09:11:01.080474Z</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DimCustomers_20251020_090909_try1",
         "DimCustomers",
         0,
         1,
         1990,
         "SUCCESS",
         "2025-10-20T09:09:15.615289Z",
         "2025-10-20T09:09:28.608432Z",
         null
        ],
        [
         "DimCustomers_20251020_091926_try1",
         "DimCustomers",
         2,
         3,
         10,
         "SUCCESS",
         "2025-10-20T09:19:29.801783Z",
         "2025-10-20T09:19:41.874974Z",
         null
        ],
        [
         "DimProducts_20251020_090913_try1",
         "DimProducts",
         0,
         1,
         490,
         "SUCCESS",
         "2025-10-20T09:09:16.627671Z",
         "2025-10-20T09:09:29.799232Z",
         null
        ],
        [
         "DimProducts_20251020_091934_try1",
         "DimProducts",
         2,
         3,
         10,
         "SUCCESS",
         "2025-10-20T09:19:38.46511Z",
         "2025-10-20T09:19:50.817749Z",
         null
        ],
        [
         "FactOrders_20251020_091037_try1",
         "FactOrders",
         0,
         1,
         9990,
         "SUCCESS",
         "2025-10-20T09:10:40.648664Z",
         "2025-10-20T09:11:01.080474Z",
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "batch_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "table_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "starting_version",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "ending_version",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "row_count",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "status",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "started_ts",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "finished_ts",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "error_msg",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 16
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "batch_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "starting_version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ending_version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "row_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "started_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "finished_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "error_msg",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select *\n",
    "from databricks_cata.gold.batch_log"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5863448154084088,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_Products",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}